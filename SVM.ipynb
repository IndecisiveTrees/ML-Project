{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61549,"databundleVersionId":6644835,"sourceType":"competition"},{"sourceId":7218242,"sourceType":"datasetVersion","datasetId":4177620}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import set_config\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score, roc_curve, make_scorer, f1_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.preprocessing import (\n    FunctionTransformer,\n    StandardScaler,\n    PowerTransformer,\n    OrdinalEncoder,\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\n# from xgboost import XGBClassifier\n# import xgboost as xgb\n# from lightgbm import LGBMClassifier\n# from catboost import CatBoostClassifier\nfrom icd9cms.icd9 import search as icdsearch\n\nsns.set_theme(style=\"white\", palette=\"viridis\")\npal = sns.color_palette(\"viridis\")\n\npd.set_option(\"display.max_rows\", 100)\nset_config(transform_output=\"pandas\")\npd.options.mode.chained_assignment = None\n\nimport gc\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nENC_CAT_COLS = [\n    \"race\",\n    \"gender\",\n    \"age\",\n    \"weight\",\n    \"payer_code\",\n    \"medical_specialty\",\n    \"diag_1\",\n    \"diag_2\",\n    \"diag_3\",\n    \"max_glu_serum\",\n    \"A1Cresult\",\n    \"metformin\",\n    \"repaglinide\",\n    \"nateglinide\",\n    \"chlorpropamide\",\n    \"glimepiride\",\n    \"acetohexamide\",\n    \"glipizide\",\n    \"glyburide\",\n    \"tolbutamide\",\n    \"pioglitazone\",\n    \"rosiglitazone\",\n    \"acarbose\",\n    \"miglitol\",\n    \"troglitazone\",\n    \"tolazamide\",\n    \"examide\",\n    \"citoglipton\",\n    \"insulin\",\n    \"glyburide-metformin\",\n    \"glipizide-metformin\",\n    \"glimepiride-pioglitazone\",\n    \"metformin-rosiglitazone\",\n    \"metformin-pioglitazone\",\n    \"change\",\n    \"diabetesMed\",\n]\nENC_CAT_COLS_NO_DRUGS = [\n    \"race\",\n    \"gender\",\n    \"age\",\n    \"payer_code\",\n    \"medical_specialty\",\n    \"diag_1\",\n    \"diag_2\",\n    \"diag_3\",\n    \"change\",\n    \"diabetesMed\",\n]\nCAT_COLS = [\n    \"enc_id\",\n    \"patient_id\",\n    \"race\",\n    \"gender\",\n    \"age\",\n    \"weight\",\n    \"admission_type_id\",\n    \"discharge_disposition_id\",\n    \"admission_source_id\",\n    \"payer_code\",\n    \"medical_specialty\",\n    \"metformin\",\n    \"repaglinide\",\n    \"nateglinide\",\n    \"chlorpropamide\",\n    \"glimepiride\",\n    \"acetohexamide\",\n    \"glipizide\",\n    \"glyburide\",\n    \"tolbutamide\",\n    \"pioglitazone\",\n    \"rosiglitazone\",\n    \"acarbose\",\n    \"miglitol\",\n    \"troglitazone\",\n    \"tolazamide\",\n    \"examide\",\n    \"citoglipton\",\n    \"insulin\",\n    \"glyburide-metformin\",\n    \"glipizide-metformin\",\n    \"glimepiride-pioglitazone\",\n    \"metformin-rosiglitazone\",\n    \"metformin-pioglitazone\",\n    \"change\",\n    \"diabetesMed\",\n    \"readmission_id\",\n]\nCAT_COLS_WITH_DIAG = [\n    \"race\",\n    \"gender\",\n    \"age\",\n    \"weight\",\n    \"admission_type_id\",\n    \"discharge_disposition_id\",\n    \"admission_source_id\",\n    \"payer_code\",\n    \"medical_specialty\",\n    \"diag_1\",\n    \"diag_2\",\n    \"diag_3\",\n    \"metformin\",\n    \"repaglinide\",\n    \"nateglinide\",\n    \"chlorpropamide\",\n    \"glimepiride\",\n    \"acetohexamide\",\n    \"glipizide\",\n    \"glyburide\",\n    \"tolbutamide\",\n    \"pioglitazone\",\n    \"rosiglitazone\",\n    \"acarbose\",\n    \"miglitol\",\n    \"troglitazone\",\n    \"tolazamide\",\n    \"examide\",\n    \"citoglipton\",\n    \"insulin\",\n    \"glyburide-metformin\",\n    \"glipizide-metformin\",\n    \"glimepiride-pioglitazone\",\n    \"metformin-rosiglitazone\",\n    \"metformin-pioglitazone\",\n    \"change\",\n    \"diabetesMed\",\n    \"readmission_id\",\n]\n\n\ndef diag(df):\n    # Initialize empty lists to store transformed values\n    diag_1 = []\n    diag_2 = []\n    diag_3 = []\n\n    # Iterate through the DataFrame to process the 'diag' columns\n    for idx, row in df.iterrows():\n        # Extract the values from 'diag_1', 'diag_2', and 'diag_3' columns\n        d1 = str(row[\"diag_1\"])\n        d2 = str(row[\"diag_2\"])\n        d3 = str(row[\"diag_3\"])\n\n        # Handle missing values (NaN)\n        if d1 == \"nan\":\n            diag_1.append(np.nan)\n        else:\n            # Process ICD9 codes that start with 'E'\n            if d1[0] == \"E\":\n                d1 = d1[:4]\n            # Truncate ICD9 codes to the first 3 characters for grouping\n            elif len(d1) > 3:\n                d1 = d1[:3]\n            # Ensure a consistent format for ICD9 codes\n            v1 = f\"{int(d1):03d}\" if d1.isnumeric() else d1\n            # Use a function 'icdsearch' (not shown in this code) to obtain a parent node for the ICD9 code\n            node = icdsearch(v1)\n            if not node:\n                print(v1, idx)\n                break\n            diag_1.append(str(node.parent))\n\n        # Repeat the same process for 'diag_2' and 'diag_3'\n        d2 = str(row[\"diag_2\"])\n        if d2 == \"nan\":\n            diag_2.append(np.nan)\n        else:\n            if d2[0 == \"E\"]:\n                d2 = d2[:4]\n            elif len(d2) > 3:\n                d2 = d2[:3]\n            v2 = f\"{int(d2):03d}\" if d2.isnumeric() else d2\n            node = icdsearch(v2)\n            if not node:\n                print(v2, idx)\n                break\n            diag_2.append(str(node.parent))\n        d3 = str(row[\"diag_3\"])\n        if d3 == \"nan\":\n            diag_3.append(np.nan)\n        else:\n            if d3[0 == \"E\"]:\n                d3 = d3[:4]\n            elif len(d3) > 3:\n                d3 = d3[:3]\n            v3 = f\"{int(d3):03d}\" if d3.isnumeric() else d3\n            node = icdsearch(v3)\n            if not node:\n                print(v3, idx)\n                break\n            diag_3.append(str(node.parent))\n\n    # Update the DataFrame with the transformed 'diag' columns\n    df[\"diag_1\"] = diag_1\n    df[\"diag_2\"] = diag_2\n    df[\"diag_3\"] = diag_3\n\n    return df\n\n\ndef drugs(data, tt):\n    # Define a nested function 'drug_changes' to process drug columns\n    def drug_changes(row):\n        d = {\"drug_up\": 0, \"drug_down\": 0, \"drug_steady\": 0}\n        for drug in row:\n            if drug == \"Up\":\n                d[\"drug_up\"] += 1\n            elif drug == \"Down\":\n                d[\"drug_down\"] += 1\n            elif drug == \"Steady\":\n                d[\"drug_steady\"] += 1\n        return pd.Series(d)\n\n    # Define a list of drug columns to be processed\n    drugs = [\n        \"metformin\",\n        \"repaglinide\",\n        \"nateglinide\",\n        \"chlorpropamide\",\n        \"glimepiride\",\n        \"acetohexamide\",\n        \"glipizide\",\n        \"glyburide\",\n        \"tolbutamide\",\n        \"pioglitazone\",\n        \"rosiglitazone\",\n        \"acarbose\",\n        \"miglitol\",\n        \"troglitazone\",\n        \"tolazamide\",\n        \"examide\",\n        \"citoglipton\",\n        \"insulin\",\n        \"glyburide-metformin\",\n        \"glipizide-metformin\",\n        \"glimepiride-pioglitazone\",\n        \"metformin-rosiglitazone\",\n        \"metformin-pioglitazone\",\n    ]\n\n    # Extract only the relevant drug columns from the data\n    drugs_data = data[drugs]\n\n    # Apply the 'drug_changes' function to create a DataFrame with drug change information\n    drug_change_df = drugs_data.apply(drug_changes, 1)\n\n    # Remove the processed drug columns from the data\n    data.drop(columns=drugs, inplace=True)\n\n    # Handle data for training and testing separately\n    if tt == \"train\":\n        # Extract the target variable 'readmission_id' for training data\n        y = data[\"readmission_id\"]\n        data.drop(columns=[\"readmission_id\"], inplace=True)\n\n        # Join the processed drug change information with the training data\n        data = data.join(drug_change_df)\n        data = data.join(y)\n    elif tt == \"test\":\n        # Join the processed drug change information with the test data\n        data = data.join(drug_change_df)\n\n    return data\n\n\ndef encode_cat(data, tt, cat_cols, enc):\n    if tt == \"train\":\n        # Encode categorical variables for the training data\n        encoded_data = pd.DataFrame(enc.fit_transform(data[cat_cols]), columns=cat_cols)\n        for col in cat_cols:\n            data[col] = encoded_data[col]\n    elif tt == \"test\":\n        # Encode categorical variables for the test data using the same encoder\n        encoded_data = pd.DataFrame(enc.transform(data[cat_cols]), columns=cat_cols)\n        for col in cat_cols:\n            data[col] = encoded_data[col]\n    return data\n\n\ndef compute_pat_cnt(data, test_data, tt, scaler):\n    # Calculate the count of admissions/re-admissions for each patient\n    vc = pd.concat([data[\"patient_id\"], test_data[\"patient_id\"]], axis=0).value_counts()\n    pat_cnt = []\n    for idx, row in data.iterrows():\n        pat_cnt.append(vc[row[\"patient_id\"]])\n\n    if tt == \"train\":\n        # Insert the 'pat_cnt' feature before the last column in the training data\n        data.insert(data.shape[1] - 1, \"pat_cnt\", pat_cnt)\n        # Standardize the 'pat_cnt' feature using scaler\n        data[\"pat_cnt\"] = scaler.fit_transform(\n            data[\"pat_cnt\"].to_numpy().reshape(-1, 1)\n        )\n    elif tt == \"test\":\n        # Insert the 'pat_cnt' feature at the end of the test data\n        data.insert(data.shape[1], \"pat_cnt\", pat_cnt)\n        data[\"pat_cnt\"] = scaler.transform(data[\"pat_cnt\"].to_numpy().reshape(-1, 1))\n\n    return data\n\n\ndef removing_null(data, tt, imputer):\n    # Drop columns with significant null values\n    data.drop(columns=[\"weight\", \"max_glu_serum\", \"A1Cresult\"], inplace=True)\n\n    # Impute constant values for 'medical_specialty' and 'payer_code'\n    data[\"medical_specialty\"] = data[\"medical_specialty\"].fillna(68)\n    data[\"payer_code\"] = data[\"payer_code\"].fillna(17)\n\n    if tt == \"train\":\n        # Extract the target variable 'readmission_id' for training data\n        y = data[\"readmission_id\"]\n\n        # Remove the target variable column\n        data = data.iloc[:, : data.shape[1] - 1]\n\n        # Impute missing values for the remaining features using the provided imputer\n        imputed_data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n\n        # Rejoin the target variable for training data\n        imputed_data = imputed_data.join(y)\n    elif tt == \"test\":\n        # Impute missing values for the test data using the same imputer\n        imputed_data = pd.DataFrame(imputer.transform(data), columns=data.columns)\n\n    return imputed_data\n\n\ndef deal_with_ids(data, tt, scaler):\n    if tt == \"train\":\n        # Normalize 'enc_id' for the training data using the provided scaler\n        data[\"enc_id\"] = scaler.fit_transform(data[\"enc_id\"].to_numpy().reshape(-1, 1))\n    elif tt == \"test\":\n        # Normalize 'enc_id' for the test data using the same scaler\n        data[\"enc_id\"] = scaler.transform(data[\"enc_id\"].to_numpy().reshape(-1, 1))\n\n    return data\n\n\ndef get_X_y(data):\n    # Extract features (X) and the target variable (y) from the given DataFrame\n    X, y = data.iloc[:, : data.shape[1] - 1], data.iloc[:, data.shape[1] - 1]\n    return X, y\n\n\ndef cv(model, X, y, params=None):\n    # Perform cross-validation of the given model\n    cv_results = cross_validate(\n        model,\n        X,\n        y,\n        scoring=[\"accuracy\", \"f1_macro\"],\n        return_estimator=True,\n        fit_params=params,\n    )\n    return cv_results\n\n\ndef gen_submission(data, model, enc_ids, fname, xg=False, np=True):\n    # Make predictions using the given model\n    if not xg:\n        if np:\n            x = data\n        else:\n            x = data.to_numpy()\n        preds = model.predict(x)\n    else:\n        x = xgb.DMatrix(data)\n        preds = model.predict(x)\n\n    if np:\n        # Create a DataFrame for submission\n        d = {\"enc_id\": enc_ids, \"readmission_id\": preds}\n        submission = pd.DataFrame(d)\n    else:\n        # Update the 'readmission_id' column with predictions\n        data[\"readmission_id\"] = preds\n        data[\"enc_id\"] = enc_ids\n        submission = data[[\"enc_id\", \"readmission_id\"]]\n\n    # Ensure data types of 'enc_id' and 'readmission_id'\n    submission.loc[:, \"enc_id\"] = submission[\"enc_id\"].astype(int)\n    submission.loc[:, \"readmission_id\"] = submission[\"readmission_id\"].astype(float)\n\n    # Save the submission DataFrame to a CSV file with the provided filename\n    submission.to_csv(fname, index=False)\n\n\ndef load_data(data_dir):\n    # Load the training and test data from the specified directory\n    data = pd.read_csv(data_dir + \"/train.csv\")\n    test_data = pd.read_csv(data_dir + \"/test.csv\")\n    return data, test_data\n\n\ndef preprocessing_and_fe(data, test_data):\n    # Extract 'enc_id' from test data for submission\n    enc_ids = test_data[\"enc_id\"]\n\n    # Process the 'diag' columns for both training and test data\n    data = diag(data)\n    test_data = diag(test_data)\n\n    # Process the 'drugs' columns for training and test data\n    data = drugs(data, \"train\")\n    test_data = drugs(test_data, \"test\")\n\n    # Encode categorical variables using OrdinalEncoder for training and test data\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n    data = encode_cat(data, \"train\", ENC_CAT_COLS_NO_DRUGS, enc)\n    test_data = encode_cat(test_data, \"test\", ENC_CAT_COLS_NO_DRUGS, enc)\n\n    # Standardize the 'pat_cnt' feature for training and test data\n    ss1 = StandardScaler()\n    data = compute_pat_cnt(data, test_data, \"train\", ss1)\n    test_data = compute_pat_cnt(test_data, data, \"test\", ss1)\n\n    # Impute missing values and handle null values for training and test data\n    imputer = SimpleImputer(strategy=\"most_frequent\")\n    data = removing_null(data, \"train\", imputer)\n    test_data = removing_null(test_data, \"test\", imputer)\n\n    # Standardize the 'enc_id' feature for training and test data\n    ss2 = StandardScaler()\n    data = deal_with_ids(data, \"train\", ss2)\n    test_data = deal_with_ids(test_data, \"test\", ss2)\n\n    # Create a list of categorical features based on the presence of 'diag' columns\n    cat_feat = list(map(lambda x: x in CAT_COLS_WITH_DIAG, data.columns.tolist()[:-1]))\n\n    # Extract X (input features) and y (target variable) for training data\n    X, y = get_X_y(data)\n\n    # Convert test data to a NumPy array\n    x = test_data\n\n    return X, y, x, enc_ids, cat_feat\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-16T19:51:05.259643Z","iopub.execute_input":"2023-12-16T19:51:05.260001Z","iopub.status.idle":"2023-12-16T19:51:05.514375Z","shell.execute_reply.started":"2023-12-16T19:51:05.259967Z","shell.execute_reply":"2023-12-16T19:51:05.513729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data, test_data = load_data(\"/kaggle/input/canadian-hospital-re-admittance-challenge/\")","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:05.515832Z","iopub.execute_input":"2023-12-16T19:51:05.516445Z","iopub.status.idle":"2023-12-16T19:51:06.201737Z","shell.execute_reply.started":"2023-12-16T19:51:05.516420Z","shell.execute_reply":"2023-12-16T19:51:06.201067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y, x, enc_ids, cat_feat = preprocessing_and_fe(data, test_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:06.203041Z","iopub.execute_input":"2023-12-16T19:51:06.203610Z","iopub.status.idle":"2023-12-16T19:51:43.831800Z","shell.execute_reply.started":"2023-12-16T19:51:06.203578Z","shell.execute_reply":"2023-12-16T19:51:43.830892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:43.833022Z","iopub.execute_input":"2023-12-16T19:51:43.833278Z","iopub.status.idle":"2023-12-16T19:51:43.840395Z","shell.execute_reply.started":"2023-12-16T19:51:43.833253Z","shell.execute_reply":"2023-12-16T19:51:43.838966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"required_columns = ['enc_id', 'patient_id', 'age', 'time_in_hospital', 'num_lab_procedures',\n       'num_procedures', 'num_medications', 'number_outpatient',\n       'number_emergency', 'number_inpatient', 'diag_1', 'diag_2', 'diag_3',\n       'number_diagnoses', 'change', 'diabetesMed', 'drug_up', 'drug_down',\n       'drug_steady', 'pat_cnt']\nX = X[required_columns]\nX.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:43.843265Z","iopub.execute_input":"2023-12-16T19:51:43.843886Z","iopub.status.idle":"2023-12-16T19:51:43.877181Z","shell.execute_reply.started":"2023-12-16T19:51:43.843845Z","shell.execute_reply":"2023-12-16T19:51:43.876528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = StandardScaler()\nX = ss.fit_transform(X)\nx = ss.transform(x[required_columns])","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:43.878154Z","iopub.execute_input":"2023-12-16T19:51:43.878571Z","iopub.status.idle":"2023-12-16T19:51:43.900182Z","shell.execute_reply.started":"2023-12-16T19:51:43.878541Z","shell.execute_reply":"2023-12-16T19:51:43.899532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, x.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:43.901012Z","iopub.execute_input":"2023-12-16T19:51:43.901851Z","iopub.status.idle":"2023-12-16T19:51:43.907146Z","shell.execute_reply.started":"2023-12-16T19:51:43.901826Z","shell.execute_reply":"2023-12-16T19:51:43.906299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:43.908168Z","iopub.execute_input":"2023-12-16T19:51:43.908431Z","iopub.status.idle":"2023-12-16T19:51:43.916023Z","shell.execute_reply.started":"2023-12-16T19:51:43.908409Z","shell.execute_reply":"2023-12-16T19:51:43.914967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.astype('float32')\nx = x.astype('float32')\nxtrain, xval, ytrain, yval = train_test_split(X, y, random_state=42,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:43.919296Z","iopub.execute_input":"2023-12-16T19:51:43.919689Z","iopub.status.idle":"2023-12-16T19:51:43.957388Z","shell.execute_reply.started":"2023-12-16T19:51:43.919661Z","shell.execute_reply":"2023-12-16T19:51:43.956657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_poly = SVC(kernel='poly', degree=3, random_state=42, verbose = True)\nsvm_poly.fit(xtrain, ytrain)\npreds = svm_poly.predict(xval)\nacc = accuracy_score(yval, preds)\nprint(f'Acc : {acc}')","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:51:43.958189Z","iopub.execute_input":"2023-12-16T19:51:43.958436Z","iopub.status.idle":"2023-12-16T19:54:45.775916Z","shell.execute_reply.started":"2023-12-16T19:51:43.958415Z","shell.execute_reply":"2023-12-16T19:54:45.774882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_rbf = SVC(kernel='rbf', random_state=42, verbose = True)\nsvm_rbf.fit(xtrain, ytrain)\npreds = svm_rbf.predict(xval)\nacc = accuracy_score(yval, preds)\nprint(f'Acc : {acc}')","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:54:45.777163Z","iopub.execute_input":"2023-12-16T19:54:45.777482Z","iopub.status.idle":"2023-12-16T19:57:56.608020Z","shell.execute_reply.started":"2023-12-16T19:54:45.777433Z","shell.execute_reply":"2023-12-16T19:57:56.607007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degrees = []\nscores = []\nfor deg in [2, 3, 5, 8, 12]:\n    svm_poly = SVC(kernel='poly', degree = deg, random_state=42, verbose = True)\n    svm_poly.fit(xtrain, ytrain)\n    preds = svm_poly.predict(xval)\n    acc = accuracy_score(yval, preds)\n    degrees.append(deg)\n    scores.append(acc)    ","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:15:39.437523Z","iopub.execute_input":"2023-12-16T20:15:39.437918Z","iopub.status.idle":"2023-12-16T20:56:29.861918Z","shell.execute_reply.started":"2023-12-16T20:15:39.437885Z","shell.execute_reply":"2023-12-16T20:56:29.860819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(x = degrees, y = scores)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:57:52.896950Z","iopub.execute_input":"2023-12-16T20:57:52.897282Z","iopub.status.idle":"2023-12-16T20:57:53.192693Z","shell.execute_reply.started":"2023-12-16T20:57:52.897255Z","shell.execute_reply":"2023-12-16T20:57:53.191521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = svm_rbf.predict(x)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T19:59:18.520010Z","iopub.execute_input":"2023-12-16T19:59:18.520339Z","iopub.status.idle":"2023-12-16T20:00:09.706252Z","shell.execute_reply.started":"2023-12-16T19:59:18.520313Z","shell.execute_reply":"2023-12-16T20:00:09.705544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.shape, np.bincount(preds), enc_ids.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:02:37.045415Z","iopub.execute_input":"2023-12-16T20:02:37.045887Z","iopub.status.idle":"2023-12-16T20:02:37.051837Z","shell.execute_reply.started":"2023-12-16T20:02:37.045855Z","shell.execute_reply":"2023-12-16T20:02:37.051240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\"enc_id\":enc_ids, \"readmission_id\": preds})\nsubmission.to_csv(\"./submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T20:04:08.154450Z","iopub.execute_input":"2023-12-16T20:04:08.154816Z","iopub.status.idle":"2023-12-16T20:04:08.191245Z","shell.execute_reply.started":"2023-12-16T20:04:08.154788Z","shell.execute_reply":"2023-12-16T20:04:08.190544Z"},"trusted":true},"execution_count":null,"outputs":[]}]}