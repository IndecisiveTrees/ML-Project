{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 1 - Canadian Hospital Re-admittance Challenge\n",
    "\n",
    "*Harsh Kumar - IMT2021016* |\n",
    "*Subhajeet Lahiri - IMT2021022* |\n",
    "*Sai Madhavan G - IMT2021101*\n",
    "\n",
    "**Note:** This file only contains clean code used for our final submission. To look at many of our attempts along the way, please look at *'./attempts.ipynb'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight, compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from joblib import dump, load\n",
    "from icd9cms.icd9 import search as icdsearch\n",
    "from sklearn.cluster import KMeans \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_CAT_COLS = ['race', 'gender', 'age', 'weight','payer_code', 'medical_specialty','diag_1',\n",
    "       'diag_2', 'diag_3', 'max_glu_serum', 'A1Cresult',\n",
    "       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
    "       'glyburide-metformin', 'glipizide-metformin',\n",
    "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "       'metformin-pioglitazone', 'change', 'diabetesMed']\n",
    "ENC_CAT_COLS_NO_DRUGS = ['race', 'gender', 'age','payer_code', 'medical_specialty','diag_1',\n",
    "       'diag_2', 'diag_3', 'change', 'diabetesMed']\n",
    "CAT_COLS = ['enc_id', 'patient_id', 'race', 'gender', 'age', 'weight',\n",
    "       'admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n",
    "       'payer_code', 'medical_specialty', \n",
    "       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
    "       'glyburide-metformin', 'glipizide-metformin',\n",
    "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "       'metformin-pioglitazone', 'change', 'diabetesMed', 'readmission_id']\n",
    "CAT_COLS_WITH_DIAG = ['race', 'gender', 'age', 'weight',\n",
    "       'admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n",
    "       'payer_code', 'medical_specialty', 'diag_1', 'diag_2', 'diag_3',\n",
    "       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
    "       'glyburide-metformin', 'glipizide-metformin',\n",
    "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "       'metformin-pioglitazone', 'change', 'diabetesMed', 'readmission_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing & Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `diag` columns\n",
    "\n",
    "The 'diag' columns contain the ICD9 codes of the primary, secondary and additional secondary diagnoses. They each contain close to 700 unique values.\n",
    "\n",
    "To reduce the number of unique values logically, we group them based on the types of disease they denote. We use the `icd9cms` library for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag(df):\n",
    "    # Initialize empty lists to store transformed values\n",
    "    diag_1 = []\n",
    "    diag_2 = []\n",
    "    diag_3 = []\n",
    "    \n",
    "    # Iterate through the DataFrame to process the 'diag' columns\n",
    "    for idx, row in df.iterrows():\n",
    "        # Extract the values from 'diag_1', 'diag_2', and 'diag_3' columns\n",
    "        d1 = str(row['diag_1'])\n",
    "        d2 = str(row['diag_2'])\n",
    "        d3 = str(row['diag_3'])\n",
    "        \n",
    "        # Handle missing values (NaN)\n",
    "        if d1 == 'nan':\n",
    "            diag_1.append(np.nan)\n",
    "        else:\n",
    "            # Process ICD9 codes that start with 'E'\n",
    "            if d1[0] == 'E':\n",
    "                d1 = d1[:4]\n",
    "            # Truncate ICD9 codes to the first 3 characters for grouping\n",
    "            elif len(d1) > 3:\n",
    "                d1 = d1[:3]\n",
    "            # Ensure a consistent format for ICD9 codes\n",
    "            v1 = f'{int(d1):03d}' if d1.isnumeric() else d1\n",
    "            # Use a function 'icdsearch' (not shown in this code) to obtain a parent node for the ICD9 code\n",
    "            node = icdsearch(v1)\n",
    "            if not node:\n",
    "                print(v1, idx)\n",
    "                break\n",
    "            diag_1.append(str(node.parent))\n",
    "        \n",
    "        # Repeat the same process for 'diag_2' and 'diag_3'\n",
    "        d2 = str(row['diag_2'])\n",
    "        if d2 == 'nan':\n",
    "            diag_2.append(np.nan)\n",
    "        else:\n",
    "            if d2[0=='E']:\n",
    "                d2 = d2[:4]\n",
    "            elif len(d2)>3:\n",
    "                d2 = d2[:3]\n",
    "            v2 = f'{int(d2):03d}' if d2.isnumeric() else d2\n",
    "            node = icdsearch(v2)\n",
    "            if not node:\n",
    "                print(v2, idx)\n",
    "                break\n",
    "            diag_2.append(str(node.parent))\n",
    "        d3 = str(row['diag_3'])\n",
    "        if d3 == 'nan':\n",
    "            diag_3.append(np.nan)\n",
    "        else:\n",
    "            if d3[0=='E']:\n",
    "                d3 = d3[:4]\n",
    "            elif len(d3)>3:\n",
    "                d3 = d3[:3]\n",
    "            v3 = f'{int(d3):03d}' if d3.isnumeric() else d3\n",
    "            node = icdsearch(v3)\n",
    "            if not node:\n",
    "                print(v3, idx)\n",
    "                break\n",
    "            diag_3.append(str(node.parent))\n",
    "        \n",
    "    # Update the DataFrame with the transformed 'diag' columns\n",
    "    df['diag_1'] = diag_1\n",
    "    df['diag_2'] = diag_2\n",
    "    df['diag_3'] = diag_3\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diag_1    684\n",
       "diag_2    691\n",
       "diag_3    745\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['diag_1', 'diag_2', 'diag_3']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diag_1    114\n",
       "diag_2    128\n",
       "diag_3    133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = diag(data)\n",
    "data[['diag_1', 'diag_2', 'diag_3']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diag_1</th>\n",
       "      <th>diag_2</th>\n",
       "      <th>diag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410-414:Ischemic Heart Disease:None</td>\n",
       "      <td>420-429:Other Forms Of Heart Disease:None</td>\n",
       "      <td>410-414:Ischemic Heart Disease:None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>410-414:Ischemic Heart Disease:None</td>\n",
       "      <td>420-429:Other Forms Of Heart Disease:None</td>\n",
       "      <td>420-429:Other Forms Of Heart Disease:None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>510-519:Other Diseases Of Respiratory System:None</td>\n",
       "      <td>510-519:Other Diseases Of Respiratory System:None</td>\n",
       "      <td>996-999:Complications Of Surgical And Medical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590-599:Other Diseases Of Urinary System:None</td>\n",
       "      <td>590-599:Other Diseases Of Urinary System:None</td>\n",
       "      <td>249-259:Diseases Of Other Endocrine Glands:None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249-259:Diseases Of Other Endocrine Glands:None</td>\n",
       "      <td>710-719:Arthropathies And Related Disorders:None</td>\n",
       "      <td>700-709:Other Diseases Of Skin And Subcutaneou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              diag_1  \\\n",
       "0                410-414:Ischemic Heart Disease:None   \n",
       "1                410-414:Ischemic Heart Disease:None   \n",
       "2  510-519:Other Diseases Of Respiratory System:None   \n",
       "3      590-599:Other Diseases Of Urinary System:None   \n",
       "4    249-259:Diseases Of Other Endocrine Glands:None   \n",
       "\n",
       "                                              diag_2  \\\n",
       "0          420-429:Other Forms Of Heart Disease:None   \n",
       "1          420-429:Other Forms Of Heart Disease:None   \n",
       "2  510-519:Other Diseases Of Respiratory System:None   \n",
       "3      590-599:Other Diseases Of Urinary System:None   \n",
       "4   710-719:Arthropathies And Related Disorders:None   \n",
       "\n",
       "                                              diag_3  \n",
       "0                410-414:Ischemic Heart Disease:None  \n",
       "1          420-429:Other Forms Of Heart Disease:None  \n",
       "2  996-999:Complications Of Surgical And Medical ...  \n",
       "3    249-259:Diseases Of Other Endocrine Glands:None  \n",
       "4  700-709:Other Diseases Of Skin And Subcutaneou...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['diag_1', 'diag_2', 'diag_3']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduces the number of unique values close to 120. \n",
    "\n",
    "We also empirically observed that this improves the performace by about 2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drugs columns\n",
    "\n",
    "There are 23 categorical columns mentioning whether that particular drug has been increased, decreased, not changed or not prescribed.\n",
    "\n",
    "We consolidate the changes in drug dosages for a particular encounter in terms of number of drugs increased, decreased or not changed in dosage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drugs(data, tt):\n",
    "    # Define a nested function 'drug_changes' to process drug columns\n",
    "    def drug_changes(row):\n",
    "        d = {\"drug_up\": 0, \"drug_down\": 0, \"drug_steady\": 0}\n",
    "        for drug in row:\n",
    "            if drug == 'Up':\n",
    "                d['drug_up'] += 1\n",
    "            elif drug == \"Down\":\n",
    "                d['drug_down'] += 1\n",
    "            elif drug == \"Steady\":\n",
    "                d['drug_steady'] += 1\n",
    "        return pd.Series(d)\n",
    "    \n",
    "    # Define a list of drug columns to be processed\n",
    "    drugs = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
    "       'glyburide-metformin', 'glipizide-metformin',\n",
    "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "       'metformin-pioglitazone']\n",
    "    \n",
    "    # Extract only the relevant drug columns from the data\n",
    "    drugs_data = data[drugs]\n",
    "    \n",
    "    # Apply the 'drug_changes' function to create a DataFrame with drug change information\n",
    "    drug_change_df = drugs_data.apply(drug_changes, 1)\n",
    "    \n",
    "    # Remove the processed drug columns from the data\n",
    "    data.drop(columns=drugs, inplace=True)\n",
    "    \n",
    "    # Handle data for training and testing separately\n",
    "    if tt == 'train':\n",
    "        # Extract the target variable 'readmission_id' for training data\n",
    "        y = data['readmission_id']\n",
    "        data.drop(columns=['readmission_id'], inplace=True)\n",
    "        \n",
    "        # Join the processed drug change information with the training data\n",
    "        data = data.join(drug_change_df)\n",
    "        data = data.join(y)\n",
    "    elif tt == 'test':\n",
    "        # Join the processed drug change information with the test data\n",
    "        data = data.join(drug_change_df)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71236, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88346340</td>\n",
       "      <td>2488608</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[60-70)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92001408</td>\n",
       "      <td>52133202</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>[100-125)</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169424316</td>\n",
       "      <td>40945509</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272987082</td>\n",
       "      <td>38850777</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[50-60)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150600612</td>\n",
       "      <td>72738225</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Down</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      enc_id  patient_id       race  gender      age     weight  \\\n",
       "0   88346340     2488608  Caucasian    Male  [60-70)        NaN   \n",
       "1   92001408    52133202  Caucasian    Male  [70-80)  [100-125)   \n",
       "2  169424316    40945509  Caucasian  Female  [70-80)        NaN   \n",
       "3  272987082    38850777  Caucasian  Female  [50-60)        NaN   \n",
       "4  150600612    72738225  Caucasian  Female  [80-90)        NaN   \n",
       "\n",
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  1                         2                    6   \n",
       "1                  2                         6                    1   \n",
       "2                  3                         2                    1   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         6                    7   \n",
       "\n",
       "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
       "0                 3  ...          No  Steady                   No   \n",
       "1                 7  ...          No      No                   No   \n",
       "2                 7  ...          No      Up                   No   \n",
       "3                 1  ...          No      No                   No   \n",
       "4                 6  ...          No    Down                   No   \n",
       "\n",
       "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
       "0                   No                        No                       No   \n",
       "1                   No                        No                       No   \n",
       "2                   No                        No                       No   \n",
       "3                   No                        No                       No   \n",
       "4                   No                        No                       No   \n",
       "\n",
       "   metformin-pioglitazone  change diabetesMed readmission_id  \n",
       "0                      No      Ch         Yes              2  \n",
       "1                      No      No         Yes              1  \n",
       "2                      No      Ch         Yes              1  \n",
       "3                      No      No         Yes              2  \n",
       "4                      No      Ch         Yes              2  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71236, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = drugs(data, 'train')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>diag_3</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>max_glu_serum</th>\n",
       "      <th>A1Cresult</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>drug_up</th>\n",
       "      <th>drug_down</th>\n",
       "      <th>drug_steady</th>\n",
       "      <th>readmission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88346340</td>\n",
       "      <td>2488608</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[60-70)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>410-414:Ischemic Heart Disease:None</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92001408</td>\n",
       "      <td>52133202</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>[100-125)</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>420-429:Other Forms Of Heart Disease:None</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169424316</td>\n",
       "      <td>40945509</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>996-999:Complications Of Surgical And Medical ...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272987082</td>\n",
       "      <td>38850777</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[50-60)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>249-259:Diseases Of Other Endocrine Glands:None</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150600612</td>\n",
       "      <td>72738225</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>700-709:Other Diseases Of Skin And Subcutaneou...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      enc_id  patient_id       race  gender      age     weight  \\\n",
       "0   88346340     2488608  Caucasian    Male  [60-70)        NaN   \n",
       "1   92001408    52133202  Caucasian    Male  [70-80)  [100-125)   \n",
       "2  169424316    40945509  Caucasian  Female  [70-80)        NaN   \n",
       "3  272987082    38850777  Caucasian  Female  [50-60)        NaN   \n",
       "4  150600612    72738225  Caucasian  Female  [80-90)        NaN   \n",
       "\n",
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  1                         2                    6   \n",
       "1                  2                         6                    1   \n",
       "2                  3                         2                    1   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         6                    7   \n",
       "\n",
       "   time_in_hospital  ...                                             diag_3  \\\n",
       "0                 3  ...                410-414:Ischemic Heart Disease:None   \n",
       "1                 7  ...          420-429:Other Forms Of Heart Disease:None   \n",
       "2                 7  ...  996-999:Complications Of Surgical And Medical ...   \n",
       "3                 1  ...    249-259:Diseases Of Other Endocrine Glands:None   \n",
       "4                 6  ...  700-709:Other Diseases Of Skin And Subcutaneou...   \n",
       "\n",
       "  number_diagnoses  max_glu_serum  A1Cresult  change  diabetesMed  drug_up  \\\n",
       "0                5            NaN        NaN      Ch          Yes        0   \n",
       "1                9            NaN        NaN      No          Yes        0   \n",
       "2                9            NaN        NaN      Ch          Yes        1   \n",
       "3                8            NaN        NaN      No          Yes        0   \n",
       "4                9            NaN        NaN      Ch          Yes        0   \n",
       "\n",
       "   drug_down drug_steady readmission_id  \n",
       "0          0           2              2  \n",
       "1          0           1              1  \n",
       "2          0           0              1  \n",
       "3          0           1              2  \n",
       "4          1           0              2  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "We encode categorical variables using `OrdinalEncoder` from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat(data, tt, cat_cols, enc):\n",
    "    if tt == 'train':\n",
    "        # Encode categorical variables for the training data\n",
    "        encoded_data = pd.DataFrame(enc.fit_transform(data[cat_cols]), columns=cat_cols)\n",
    "        for col in cat_cols:\n",
    "            data[col] = encoded_data[col]\n",
    "    elif tt == 'test':\n",
    "        # Encode categorical variables for the test data using the same encoder\n",
    "        encoded_data = pd.DataFrame(enc.transform(data[cat_cols]), columns=cat_cols)\n",
    "        for col in cat_cols:\n",
    "            data[col] = encoded_data[col]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>diag_3</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>max_glu_serum</th>\n",
       "      <th>A1Cresult</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>drug_up</th>\n",
       "      <th>drug_down</th>\n",
       "      <th>drug_steady</th>\n",
       "      <th>readmission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88346340</td>\n",
       "      <td>2488608</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92001408</td>\n",
       "      <td>52133202</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[100-125)</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169424316</td>\n",
       "      <td>40945509</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>106.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272987082</td>\n",
       "      <td>38850777</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150600612</td>\n",
       "      <td>72738225</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      enc_id  patient_id  race  gender  age     weight  admission_type_id  \\\n",
       "0   88346340     2488608   2.0     1.0  6.0        NaN                  1   \n",
       "1   92001408    52133202   2.0     1.0  7.0  [100-125)                  2   \n",
       "2  169424316    40945509   2.0     0.0  7.0        NaN                  3   \n",
       "3  272987082    38850777   2.0     0.0  5.0        NaN                  1   \n",
       "4  150600612    72738225   2.0     0.0  8.0        NaN                  1   \n",
       "\n",
       "   discharge_disposition_id  admission_source_id  time_in_hospital  ...  \\\n",
       "0                         2                    6                 3  ...   \n",
       "1                         6                    1                 7  ...   \n",
       "2                         2                    1                 7  ...   \n",
       "3                         1                    7                 1  ...   \n",
       "4                         6                    7                 6  ...   \n",
       "\n",
       "   diag_3  number_diagnoses  max_glu_serum  A1Cresult  change  diabetesMed  \\\n",
       "0    44.0                 5            NaN        NaN     0.0          1.0   \n",
       "1    46.0                 9            NaN        NaN     1.0          1.0   \n",
       "2   106.0                 9            NaN        NaN     0.0          1.0   \n",
       "3    26.0                 8            NaN        NaN     1.0          1.0   \n",
       "4    75.0                 9            NaN        NaN     0.0          1.0   \n",
       "\n",
       "   drug_up  drug_down  drug_steady  readmission_id  \n",
       "0        0          0            2               2  \n",
       "1        0          0            1               1  \n",
       "2        1          0            0               1  \n",
       "3        0          0            1               2  \n",
       "4        0          1            0               2  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    "data = encode_cat(data, 'train', ENC_CAT_COLS_NO_DRUGS, enc)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new feature: `pat_cnt`\n",
    "\n",
    "We create a new feature `pat_cnt` (patient count) for looking at how many times a patient has been admitted/re-admitted using duplicate values of `patient_id`. \n",
    "\n",
    "This turned out to be the single most useful feature in the dataset increasing the score by **15%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pat_cnt(data, test_data, tt, scaler):\n",
    "    # Calculate the count of admissions/re-admissions for each patient\n",
    "    vc = pd.concat([data['patient_id'], test_data['patient_id']], axis=0).value_counts()\n",
    "    pat_cnt = []\n",
    "    for idx, row in data.iterrows():\n",
    "        pat_cnt.append(vc[row['patient_id']])\n",
    "    \n",
    "    if tt == 'train':\n",
    "        # Insert the 'pat_cnt' feature before the last column in the training data\n",
    "        data.insert(data.shape[1] - 1, 'pat_cnt', pat_cnt)\n",
    "        # Standardize the 'pat_cnt' feature using scaler\n",
    "        data['pat_cnt'] = scaler.fit_transform(data['pat_cnt'].to_numpy().reshape(-1, 1))\n",
    "    elif tt == 'test':\n",
    "        # Insert the 'pat_cnt' feature at the end of the test data\n",
    "        data.insert(data.shape[1], 'pat_cnt', pat_cnt)\n",
    "        data['pat_cnt'] = scaler.transform(data['pat_cnt'].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.514042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.105474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.345931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.514042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.514042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pat_cnt\n",
       "0 -0.514042\n",
       "1 -0.105474\n",
       "2  2.345931\n",
       "3 -0.514042\n",
       "4 -0.514042"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss1 = StandardScaler()\n",
    "data = compute_pat_cnt(data, test_data, 'train', ss1)\n",
    "\n",
    "data[['pat_cnt']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight                      96.841485\n",
       "max_glu_serum               94.776517\n",
       "A1Cresult                   83.323039\n",
       "medical_specialty           49.034196\n",
       "payer_code                  39.555843\n",
       "race                         2.275535\n",
       "diag_3                       1.388343\n",
       "diag_2                       0.342523\n",
       "diag_1                       0.021057\n",
       "number_diagnoses             0.000000\n",
       "enc_id                       0.000000\n",
       "change                       0.000000\n",
       "drug_up                      0.000000\n",
       "drug_down                    0.000000\n",
       "drug_steady                  0.000000\n",
       "pat_cnt                      0.000000\n",
       "diabetesMed                  0.000000\n",
       "number_outpatient            0.000000\n",
       "number_inpatient             0.000000\n",
       "number_emergency             0.000000\n",
       "patient_id                   0.000000\n",
       "num_medications              0.000000\n",
       "num_procedures               0.000000\n",
       "num_lab_procedures           0.000000\n",
       "time_in_hospital             0.000000\n",
       "admission_source_id          0.000000\n",
       "discharge_disposition_id     0.000000\n",
       "admission_type_id            0.000000\n",
       "age                          0.000000\n",
       "gender                       0.000000\n",
       "readmission_id               0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.isna().sum()/len(data)*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that several columns have a significant number of null values\n",
    "\n",
    "We employ the following strategies for dealing with null values based on experimentation:\n",
    "1. Dropping columns: `weight`, `max_glu_serum`, `A1Cresult`\n",
    "2. Imputing constant value: `medical_specialty`, `payer_code`\n",
    "3. Imputing the mode: `race`, `diag_*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_null(data, tt, imputer):\n",
    "    # Drop columns with significant null values\n",
    "    data.drop(columns=['weight', 'max_glu_serum', 'A1Cresult'], inplace=True)\n",
    "    \n",
    "    # Impute constant values for 'medical_specialty' and 'payer_code'\n",
    "    data['medical_specialty'] = data['medical_specialty'].fillna(68)\n",
    "    data['payer_code'] = data['payer_code'].fillna(17)\n",
    "    \n",
    "    if tt == \"train\":\n",
    "        # Extract the target variable 'readmission_id' for training data\n",
    "        y = data['readmission_id']\n",
    "        \n",
    "        # Remove the target variable column\n",
    "        data = data.iloc[:, :data.shape[1] - 1]\n",
    "        \n",
    "        # Impute missing values for the remaining features using the provided imputer\n",
    "        imputed_data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "        \n",
    "        # Rejoin the target variable for training data\n",
    "        imputed_data = imputed_data.join(y)\n",
    "    elif tt == \"test\":\n",
    "        # Impute missing values for the test data using the same imputer\n",
    "        imputed_data = pd.DataFrame(imputer.transform(data), columns=data.columns)\n",
    "    \n",
    "    return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enc_id                      0.0\n",
       "patient_id                  0.0\n",
       "pat_cnt                     0.0\n",
       "drug_steady                 0.0\n",
       "drug_down                   0.0\n",
       "drug_up                     0.0\n",
       "diabetesMed                 0.0\n",
       "change                      0.0\n",
       "number_diagnoses            0.0\n",
       "diag_3                      0.0\n",
       "diag_2                      0.0\n",
       "diag_1                      0.0\n",
       "number_inpatient            0.0\n",
       "number_emergency            0.0\n",
       "number_outpatient           0.0\n",
       "num_medications             0.0\n",
       "num_procedures              0.0\n",
       "num_lab_procedures          0.0\n",
       "medical_specialty           0.0\n",
       "payer_code                  0.0\n",
       "time_in_hospital            0.0\n",
       "admission_source_id         0.0\n",
       "discharge_disposition_id    0.0\n",
       "admission_type_id           0.0\n",
       "age                         0.0\n",
       "gender                      0.0\n",
       "race                        0.0\n",
       "readmission_id              0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data = removing_null(data, 'train', imputer)\n",
    "\n",
    "(data.isna().sum()/len(data)*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pecularity with `id`'s\n",
    "\n",
    "We observed how `pat_cnt` feature drastically improved our results, we wondered if there was some implicit pattern with the id's which might help us improve our results.\n",
    "\n",
    "This made us realize that normalizing the value of `enc_id` gives us a significant boost to the score\n",
    "\n",
    "`patient_id` performed better without normalizing however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_ids(data, tt, scaler):\n",
    "    if tt == 'train':\n",
    "        # Normalize 'enc_id' for the training data using the provided scaler\n",
    "        data['enc_id'] = scaler.fit_transform(data['enc_id'].to_numpy().reshape(-1, 1))\n",
    "    elif tt == 'test':\n",
    "        # Normalize 'enc_id' for the test data using the same scaler\n",
    "        data['enc_id'] = scaler.transform(data['enc_id'].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a function to get training data as X & y numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(data):\n",
    "    # Extract features (X) and the target variable (y) from the given DataFrame\n",
    "    X, y = data.iloc[:, :data.shape[1] - 1].to_numpy(), data.iloc[:, data.shape[1] - 1].to_numpy()\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a function to cross validate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(model, X, y, params=None):\n",
    "    # Perform cross-validation of the given model\n",
    "    cv_results = cross_validate(model, X, y, scoring=['accuracy', 'f1_macro'], return_estimator=True, fit_params=params)\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a function to generate a submission file in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_submission(data, model, enc_ids, fname, xg=False, np=True):\n",
    "    # Make predictions using the given model\n",
    "    if not xg:\n",
    "        if np:\n",
    "            x = data\n",
    "        else:\n",
    "            x = data.to_numpy()\n",
    "        preds = model.predict(x)\n",
    "    else:\n",
    "        x = xgb.DMatrix(data)\n",
    "        preds = model.predict(x)\n",
    "    \n",
    "    if np:\n",
    "        # Create a DataFrame for submission\n",
    "        d = {'enc_id': enc_ids, 'readmission_id': preds}\n",
    "        submission = pd.DataFrame(d)\n",
    "    else:\n",
    "        # Update the 'readmission_id' column with predictions\n",
    "        data['readmission_id'] = preds\n",
    "        data['enc_id'] = enc_ids\n",
    "        submission = data[['enc_id', 'readmission_id']]\n",
    "    \n",
    "    # Ensure data types of 'enc_id' and 'readmission_id'\n",
    "    submission.loc[:, 'enc_id'] = submission['enc_id'].astype(int)\n",
    "    submission.loc[:, 'readmission_id'] = submission['readmission_id'].astype(float)\n",
    "    \n",
    "    # Save the submission DataFrame to a CSV file with the provided filename\n",
    "    submission.to_csv(fname, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encapsulate all the above functons in a couple of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    # Load the training and test data from the specified directory\n",
    "    data = pd.read_csv(data_dir + '/train.csv')\n",
    "    test_data = pd.read_csv(data_dir + '/test.csv')\n",
    "    return data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_and_fe(data, test_data):\n",
    "    # Extract 'enc_id' from test data for submission\n",
    "    enc_ids = test_data['enc_id']\n",
    "    \n",
    "    # Process the 'diag' columns for both training and test data\n",
    "    data = diag(data)\n",
    "    test_data = diag(test_data)\n",
    "    \n",
    "    # Process the 'drugs' columns for training and test data\n",
    "    data = drugs(data, 'train')\n",
    "    test_data = drugs(test_data, 'test')\n",
    "    \n",
    "    # Encode categorical variables using OrdinalEncoder for training and test data\n",
    "    enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    "    data = encode_cat(data, 'train', ENC_CAT_COLS_NO_DRUGS, enc)\n",
    "    test_data = encode_cat(test_data, 'test', ENC_CAT_COLS_NO_DRUGS, enc)\n",
    "    \n",
    "    # Standardize the 'pat_cnt' feature for training and test data\n",
    "    ss1 = StandardScaler()\n",
    "    data = compute_pat_cnt(data, test_data, 'train', ss1)\n",
    "    test_data = compute_pat_cnt(test_data, data, 'test', ss1)\n",
    "    \n",
    "    # Impute missing values and handle null values for training and test data\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data = removing_null(data, 'train', imputer)\n",
    "    test_data = removing_null(test_data, 'test', imputer)\n",
    "    \n",
    "    # Standardize the 'enc_id' feature for training and test data\n",
    "    ss2 = StandardScaler()\n",
    "    data = deal_with_ids(data, 'train', ss2)\n",
    "    test_data = deal_with_ids(test_data, 'test', ss2)\n",
    "\n",
    "    # Create a list of categorical features based on the presence of 'diag' columns\n",
    "    cat_feat = list(map(lambda x: x in CAT_COLS_WITH_DIAG, data.columns.tolist()[:-1]))\n",
    "    \n",
    "    # Extract X (input features) and y (target variable) for training data\n",
    "    X, y = get_X_y(data)\n",
    "    \n",
    "    # Convert test data to a NumPy array\n",
    "    x = test_data.to_numpy()\n",
    "    \n",
    "    return X, y, x, enc_ids, cat_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model used\n",
    "\n",
    "We use the `HistGradientBoostClassifier` classifier from sklearn which is a variant of Gradient Boosting inspired by LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1: 0.5522265857269146\n"
     ]
    }
   ],
   "source": [
    "data, test_data = load_data('data')\n",
    "\n",
    "# Load and preprocess the data\n",
    "X, y, x, enc_ids, cat_feat = preprocessing_and_fe(data, test_data)\n",
    "\n",
    "# Initialize the HistGradientBoostingClassifier with categorical features\n",
    "hgbc = HistGradientBoostingClassifier(categorical_features=cat_feat)\n",
    "\n",
    "# Perform cross-validation to find the best f1 score\n",
    "cv_res = cv(hgbc, X, y)\n",
    "best_f1 = max(cv_res['test_f1_macro'])\n",
    "print(f'Best f1: {best_f1}')\n",
    "\n",
    "# Get the model with the best f1 score\n",
    "model = cv_res['estimator'][cv_res['test_f1_macro'].argmax()]\n",
    "\n",
    "# Generate the final submission\n",
    "gen_submission(x, model, enc_ids, 'final_submission.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tuning hyper parameters, we first do random search on several possible combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=HistGradientBoostingClassifier(categorical_features=[False,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False]),\n",
       "                   n_iter=30, n_jobs=-1,\n",
       "                   param_distributions={&#x27;l2_regularization&#x27;: [0, 1, 1.5],\n",
       "                                        &#x27;max_iter&#x27;: [100, 1000, 3000],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [10, 50, 100],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [20, 50, 10],\n",
       "                                        &#x27;scoring&#x27;: [&#x27;f1_macro&#x27;]},\n",
       "                   scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=HistGradientBoostingClassifier(categorical_features=[False,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False]),\n",
       "                   n_iter=30, n_jobs=-1,\n",
       "                   param_distributions={&#x27;l2_regularization&#x27;: [0, 1, 1.5],\n",
       "                                        &#x27;max_iter&#x27;: [100, 1000, 3000],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [10, 50, 100],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [20, 50, 10],\n",
       "                                        &#x27;scoring&#x27;: [&#x27;f1_macro&#x27;]},\n",
       "                   scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier(categorical_features=[False, False, True, True,\n",
       "                                                     True, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False, False,\n",
       "                                                     False, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier(categorical_features=[False, False, True, True,\n",
       "                                                     True, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False, False,\n",
       "                                                     False, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False])</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=HistGradientBoostingClassifier(categorical_features=[False,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  True,\n",
       "                                                                                  True,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False,\n",
       "                                                                                  False]),\n",
       "                   n_iter=30, n_jobs=-1,\n",
       "                   param_distributions={'l2_regularization': [0, 1, 1.5],\n",
       "                                        'max_iter': [100, 1000, 3000],\n",
       "                                        'max_leaf_nodes': [10, 50, 100],\n",
       "                                        'min_samples_leaf': [20, 50, 10],\n",
       "                                        'scoring': ['f1_macro']},\n",
       "                   scoring='f1_macro')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HistGradientBoostingClassifier(categorical_features=cat_feat)\n",
    "\n",
    "# Define a dictionary of hyperparameter options to search\n",
    "param_distributions = {\n",
    "    'max_iter': [100, 1000, 3000],\n",
    "    'max_leaf_nodes': [10, 50, 100],\n",
    "    'min_samples_leaf': [20, 50, 10],\n",
    "    'l2_regularization': [0, 1, 1.5],\n",
    "    'scoring': ['f1_macro'],\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV object to search for the best hyperparameters\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, n_jobs=-1, cv=3, scoring='f1_macro', n_iter=30\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the training data\n",
    "model_random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_scoring</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_max_leaf_nodes</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_l2_regularization</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.979763</td>\n",
       "      <td>9.888138</td>\n",
       "      <td>1.213187</td>\n",
       "      <td>0.272197</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'scoring': 'f1_macro', 'min_samples_leaf': 50...</td>\n",
       "      <td>0.554039</td>\n",
       "      <td>0.553747</td>\n",
       "      <td>0.554298</td>\n",
       "      <td>0.554028</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28.103802</td>\n",
       "      <td>6.140377</td>\n",
       "      <td>1.103143</td>\n",
       "      <td>0.185629</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'scoring': 'f1_macro', 'min_samples_leaf': 10...</td>\n",
       "      <td>0.553418</td>\n",
       "      <td>0.554280</td>\n",
       "      <td>0.554095</td>\n",
       "      <td>0.553931</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>36.374589</td>\n",
       "      <td>9.653315</td>\n",
       "      <td>1.005154</td>\n",
       "      <td>0.114661</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>{'scoring': 'f1_macro', 'min_samples_leaf': 50...</td>\n",
       "      <td>0.556574</td>\n",
       "      <td>0.551948</td>\n",
       "      <td>0.553028</td>\n",
       "      <td>0.553850</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31.820839</td>\n",
       "      <td>10.879679</td>\n",
       "      <td>1.174793</td>\n",
       "      <td>0.260457</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'scoring': 'f1_macro', 'min_samples_leaf': 50...</td>\n",
       "      <td>0.556023</td>\n",
       "      <td>0.554635</td>\n",
       "      <td>0.550315</td>\n",
       "      <td>0.553658</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>26.538475</td>\n",
       "      <td>4.989785</td>\n",
       "      <td>1.097434</td>\n",
       "      <td>0.243954</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'scoring': 'f1_macro', 'min_samples_leaf': 50...</td>\n",
       "      <td>0.557354</td>\n",
       "      <td>0.547305</td>\n",
       "      <td>0.555357</td>\n",
       "      <td>0.553339</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "9       32.979763      9.888138         1.213187        0.272197   \n",
       "6       28.103802      6.140377         1.103143        0.185629   \n",
       "22      36.374589      9.653315         1.005154        0.114661   \n",
       "18      31.820839     10.879679         1.174793        0.260457   \n",
       "17      26.538475      4.989785         1.097434        0.243954   \n",
       "\n",
       "   param_scoring param_min_samples_leaf param_max_leaf_nodes param_max_iter  \\\n",
       "9       f1_macro                     50                  100           3000   \n",
       "6       f1_macro                     10                  100           1000   \n",
       "22      f1_macro                     50                   50           1000   \n",
       "18      f1_macro                     50                  100            100   \n",
       "17      f1_macro                     50                   50           3000   \n",
       "\n",
       "   param_l2_regularization                                             params  \\\n",
       "9                      1.5  {'scoring': 'f1_macro', 'min_samples_leaf': 50...   \n",
       "6                        1  {'scoring': 'f1_macro', 'min_samples_leaf': 10...   \n",
       "22                       0  {'scoring': 'f1_macro', 'min_samples_leaf': 50...   \n",
       "18                     1.5  {'scoring': 'f1_macro', 'min_samples_leaf': 50...   \n",
       "17                       1  {'scoring': 'f1_macro', 'min_samples_leaf': 50...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "9            0.554039           0.553747           0.554298         0.554028   \n",
       "6            0.553418           0.554280           0.554095         0.553931   \n",
       "22           0.556574           0.551948           0.553028         0.553850   \n",
       "18           0.556023           0.554635           0.550315         0.553658   \n",
       "17           0.557354           0.547305           0.555357         0.553339   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "9         0.000225                1  \n",
       "6         0.000371                2  \n",
       "22        0.001976                3  \n",
       "18        0.002431                4  \n",
       "17        0.004344                5  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the cross-validation results and sort them by mean test score\n",
    "cv_results = pd.DataFrame(model_random_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False\n",
    ")\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a grid search on the resulting hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=HistGradientBoostingClassifier(categorical_features=[False,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;l2_regularization&#x27;: [1, 1.5],\n",
       "                         &#x27;max_iter&#x27;: [1000, 3000], &#x27;max_leaf_nodes&#x27;: [100, 150],\n",
       "                         &#x27;min_samples_leaf&#x27;: [50, 10],\n",
       "                         &#x27;scoring&#x27;: [&#x27;f1_macro&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=HistGradientBoostingClassifier(categorical_features=[False,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;l2_regularization&#x27;: [1, 1.5],\n",
       "                         &#x27;max_iter&#x27;: [1000, 3000], &#x27;max_leaf_nodes&#x27;: [100, 150],\n",
       "                         &#x27;min_samples_leaf&#x27;: [50, 10],\n",
       "                         &#x27;scoring&#x27;: [&#x27;f1_macro&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier(categorical_features=[False, False, True, True,\n",
       "                                                     True, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False, False,\n",
       "                                                     False, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier(categorical_features=[False, False, True, True,\n",
       "                                                     True, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False, False,\n",
       "                                                     False, True, True, True,\n",
       "                                                     False, True, True, False,\n",
       "                                                     False, False, False])</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=HistGradientBoostingClassifier(categorical_features=[False,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            True,\n",
       "                                                                            True,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False,\n",
       "                                                                            False]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'l2_regularization': [1, 1.5],\n",
       "                         'max_iter': [1000, 3000], 'max_leaf_nodes': [100, 150],\n",
       "                         'min_samples_leaf': [50, 10],\n",
       "                         'scoring': ['f1_macro']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_iter': [1000, 3000],\n",
    "    'max_leaf_nodes': [100, 150],\n",
    "    'min_samples_leaf': [50, 10],\n",
    "    'l2_regularization': [1, 1.5],\n",
    "    'scoring': ['f1_macro'],\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object to search for the best hyperparameters\n",
    "model_grid_search = GridSearchCV(\n",
    "    model, param_grid=param_grid, n_jobs=-1, cv=3, scoring='f1_macro'\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "model_grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_l2_regularization</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_max_leaf_nodes</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_scoring</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33.150123</td>\n",
       "      <td>1.324454</td>\n",
       "      <td>0.763672</td>\n",
       "      <td>0.025422</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3000</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>{'l2_regularization': 1.5, 'max_iter': 3000, '...</td>\n",
       "      <td>0.556349</td>\n",
       "      <td>0.552617</td>\n",
       "      <td>0.559853</td>\n",
       "      <td>0.556273</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.460158</td>\n",
       "      <td>7.595268</td>\n",
       "      <td>1.165390</td>\n",
       "      <td>0.240434</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>{'l2_regularization': 1, 'max_iter': 1000, 'ma...</td>\n",
       "      <td>0.557672</td>\n",
       "      <td>0.553547</td>\n",
       "      <td>0.555623</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.882890</td>\n",
       "      <td>6.033982</td>\n",
       "      <td>1.161958</td>\n",
       "      <td>0.174209</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>{'l2_regularization': 1, 'max_iter': 3000, 'ma...</td>\n",
       "      <td>0.559500</td>\n",
       "      <td>0.551789</td>\n",
       "      <td>0.554734</td>\n",
       "      <td>0.555341</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38.837281</td>\n",
       "      <td>4.321131</td>\n",
       "      <td>1.464876</td>\n",
       "      <td>0.275889</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>{'l2_regularization': 1.5, 'max_iter': 1000, '...</td>\n",
       "      <td>0.556790</td>\n",
       "      <td>0.551971</td>\n",
       "      <td>0.555961</td>\n",
       "      <td>0.554907</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37.176221</td>\n",
       "      <td>9.488804</td>\n",
       "      <td>1.541032</td>\n",
       "      <td>0.202772</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>{'l2_regularization': 1, 'max_iter': 3000, 'ma...</td>\n",
       "      <td>0.558064</td>\n",
       "      <td>0.550115</td>\n",
       "      <td>0.555947</td>\n",
       "      <td>0.554709</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14      33.150123      1.324454         0.763672        0.025422   \n",
       "0       32.460158      7.595268         1.165390        0.240434   \n",
       "5       30.882890      6.033982         1.161958        0.174209   \n",
       "10      38.837281      4.321131         1.464876        0.275889   \n",
       "6       37.176221      9.488804         1.541032        0.202772   \n",
       "\n",
       "   param_l2_regularization param_max_iter param_max_leaf_nodes  \\\n",
       "14                     1.5           3000                  150   \n",
       "0                        1           1000                  100   \n",
       "5                        1           3000                  100   \n",
       "10                     1.5           1000                  150   \n",
       "6                        1           3000                  150   \n",
       "\n",
       "   param_min_samples_leaf param_scoring  \\\n",
       "14                     50      f1_macro   \n",
       "0                      50      f1_macro   \n",
       "5                      10      f1_macro   \n",
       "10                     50      f1_macro   \n",
       "6                      50      f1_macro   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "14  {'l2_regularization': 1.5, 'max_iter': 3000, '...           0.556349   \n",
       "0   {'l2_regularization': 1, 'max_iter': 1000, 'ma...           0.557672   \n",
       "5   {'l2_regularization': 1, 'max_iter': 3000, 'ma...           0.559500   \n",
       "10  {'l2_regularization': 1.5, 'max_iter': 1000, '...           0.556790   \n",
       "6   {'l2_regularization': 1, 'max_iter': 3000, 'ma...           0.558064   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "14           0.552617           0.559853         0.556273        0.002955   \n",
       "0            0.553547           0.555623         0.555614        0.001684   \n",
       "5            0.551789           0.554734         0.555341        0.003177   \n",
       "10           0.551971           0.555961         0.554907        0.002103   \n",
       "6            0.550115           0.555947         0.554709        0.003361   \n",
       "\n",
       "    rank_test_score  \n",
       "14                1  \n",
       "0                 2  \n",
       "5                 3  \n",
       "10                4  \n",
       "6                 5  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the cross-validation results and sort them by mean test score\n",
    "cv_results_gs = pd.DataFrame(model_grid_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False\n",
    ")\n",
    "cv_results_gs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model with the optimized hyperparameters\n",
    "best_model = model_grid_search.best_estimator_\n",
    "\n",
    "# Generate a submission using the best model\n",
    "gen_submission(x, best_model, enc_ids, 'final_hyp_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
